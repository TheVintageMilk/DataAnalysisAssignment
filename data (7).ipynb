{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Running application"
      ],
      "metadata": {
        "id": "mNd4O34C1P09"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nocL42qz5AwM"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "%cd '/content/drive/MyDrive/data assignment/merging'\n",
        "\n",
        "%ls\n",
        "\n",
        "!pip install streamlit\n",
        "\n",
        "!wget -q -O - ipv4.icanhazip.com\n",
        "\n",
        "\n",
        "!streamlit run app.py & npx localtunnel --port 8501"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Merging used datasets"
      ],
      "metadata": {
        "id": "7jrEJvvd7WwO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Define the specific file paths and their location types\n",
        "files_info = {\n",
        "    \"/content/merging/PRSA_Data_Dingling_20130301-20170228.csv\": \"Rural\",\n",
        "    \"/content/merging/PRSA_Data_Gucheng_20130301-20170228.csv\": \"Suburban\",\n",
        "    \"/content/merging/PRSA_Data_Nongzhanguan_20130301-20170228.csv\": \"Industrial\",\n",
        "    \"/content/merging/PRSA_Data_Tiantan_20130301-20170228.csv\": \"Urban\"\n",
        "}\n",
        "\n",
        "# Read and label each file\n",
        "dataframes = []\n",
        "for path, location in files_info.items():\n",
        "    df = pd.read_csv(path)\n",
        "    df['LocationType'] = location  # Add a column to tag location type\n",
        "    dataframes.append(df)\n",
        "\n",
        "# Merge all into a single DataFrame\n",
        "merged_df = pd.concat(dataframes, ignore_index=True)\n",
        "\n",
        "# Save the merged dataset to your workspace\n",
        "merged_df.to_csv(\"/content/merged_PRSA_data.csv\", index=False)\n",
        "missing = df.isnull().sum()\n",
        "\n",
        "print(\"Files successfully merged and saved as 'merged_PRSA_data.csv'\")\n"
      ],
      "metadata": {
        "id": "Gj7bWvfm7dXH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Displaying Data showing total errors"
      ],
      "metadata": {
        "id": "lzFd0aQD8FNx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Load the merged dataset\n",
        "df = pd.read_csv('/content/drive/MyDrive/data assignment/merging/merged_PRSA_data.csv')\n",
        "\n",
        "\n",
        "# Check how many missing values per column along with other info\n",
        "missing = df.isnull().sum()\n",
        "\n",
        "print(\"Number of rows and columns:\\n\", df.shape)\n",
        "print(\"Column names and data types:\\n\", df.dtypes)\n",
        "print(\"Sample of dataset:\\n\", df.head())\n",
        "print(\"Missing values:\\n\", missing)\n",
        "\n",
        "# Visualise the missing data\n",
        "import missingno as msno\n",
        "msno.matrix(df)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "UN4zOSfo8Ip1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hybrid clean up of data"
      ],
      "metadata": {
        "id": "hRJqc5K08uh9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "#missingno for null visualization\n",
        "import missingno as msno\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_csv('/content/merged_PRSA_data.csv')\n",
        "print(\"Initial shape:\", df.shape)\n",
        "\n",
        "# define data column types\n",
        "critical_columns = ['PM2.5', 'PM10', 'SO2', 'NO2', 'CO', 'O3', 'wd']\n",
        "weather_columns = ['RAIN', 'TEMP', 'DEWP', 'PRES', 'WSPM']\n",
        "\n",
        "# Drop rows where important pollutants are missing\n",
        "df = df.dropna(subset=critical_columns)\n",
        "\n",
        "# Fill missing weather data with median value\n",
        "for col in weather_columns:\n",
        "    if col in df.columns:\n",
        "        median_value = df[col].median()\n",
        "        df[col] = df[col].fillna(median_value)\n",
        "\n",
        "# Double-check if any missing values left\n",
        "print(\"\\nMissing values after cleaning:\")\n",
        "print(df.isnull().sum())\n",
        "\n",
        "# Visualise the missing data\n",
        "import missingno as msno\n",
        "msno.matrix(df)\n",
        "plt.show()\n",
        "\n",
        "df.to_csv('merged_cleaned_PRSA_data.csv', index=False)\n",
        "\n",
        "print(\"\\nData fully cleaned and saved\")"
      ],
      "metadata": {
        "id": "Jx-mi3YA8xht"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "EDA and correlation"
      ],
      "metadata": {
        "id": "ii1_bJA99NBl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Load in the clean data\n",
        "df = pd.read_csv('merged_cleaned_PRSA_data.csv')\n",
        "\n",
        "print(df.info())\n",
        "\n",
        "relevant_columns = [\n",
        "    'PM2.5', 'PM10', 'SO2', 'NO2', 'CO', 'O3',\n",
        "    'TEMP', 'PRES', 'DEWP', 'RAIN', 'WSPM'\n",
        "]\n",
        "\n",
        "x_labels = [\n",
        "    \"PM2.5 Concentration (μg/m³)\",     # PM2.5\n",
        "    \"PM10 Concentration (μg/m³)\",      # PM10\n",
        "    \"SO₂ Concentration (μg/m³)\",       # SO2\n",
        "    \"NO₂ Concentration (μg/m³)\",       # NO2\n",
        "    \"CO Concentration (mg/m³)\",        # CO\n",
        "    \"O₃ Concentration (μg/m³)\",        # O3\n",
        "    \"Temperature (°C)\",                # TEMP\n",
        "    \"Pressure (hPa)\",                  # PRES\n",
        "    \"Dew Point (°C)\",                  # DEWP\n",
        "    \"Rainfall (mm)\",                   # RAIN\n",
        "    \"Wind Speed (m/s)\"                 # WSPM\n",
        "]\n",
        "\n",
        "\n",
        "#  Summary Statistics\n",
        "print(\"\\nSummary Statistics:\\n\")\n",
        "print(df[relevant_columns].describe().round(2))\n",
        "\n",
        "#  Histograms for distribution\n",
        "print(\"\\nHistograms:\\n\")\n",
        "df[relevant_columns].hist(bins=30, figsize=(15, 10), color='skyblue', edgecolor='black')\n",
        "plt.suptitle('Histograms of Relevant Variables', fontsize=18)\n",
        "plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
        "plt.show()\n",
        "\n",
        "# First, import the function from scipy\n",
        "from scipy.stats import skew\n",
        "\n",
        "# Loop through and print skewness\n",
        "for col in relevant_columns:\n",
        "    skewness = skew(df[col].dropna())  # dropna() to avoid issues with missing values\n",
        "    print(f\"Skewness for {col}: {skewness:.2f}\")\n",
        "\n",
        "print(\"\\n Insight: The distribution of pollutant levels (PM2.5, PM10, CO, NO2) is right-skewed, suggesting that extreme pollution events occur less frequently but have a significant impact.\")\n",
        "\n",
        "#  Boxplots to spot outliers\n",
        "print(\"\\nBoxplots:\\n\")\n",
        "plots_per_row = 3\n",
        "\n",
        "for i in range(0, len(relevant_columns), plots_per_row):\n",
        "    fig, axes = plt.subplots(1, plots_per_row, figsize=(18, 5))  # 1 row, 3 columns\n",
        "\n",
        "    for j in range(plots_per_row):\n",
        "        if i + j < len(relevant_columns):\n",
        "            sns.boxplot(data=df, x=relevant_columns[i + j], ax=axes[j])\n",
        "            axes[j].set_title(f'Boxplot of {relevant_columns[i + j]}')\n",
        "            axes[j].set_xlabel(x_labels[i + j])\n",
        "        else:\n",
        "            axes[j].set_visible(False)  # Hide empty subplots if any\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "print(\"\\n Insight: The boxplots revealed that variables such as PM2.5, PM10, SO2, NO2, CO, O3, Rain, and WSPM exhibited a large number of outliers, especially \\n toward higher values, indicating right-skewed distributions and significant variability.\"\n",
        "\"\\n In contrast, meteorological variables such as Temperature (TEMP), Pressure (PRES), and Dew Point (DEWP) showed relatively symmetrical distributions with \\n fewer outliers, suggesting more stable behavior across the dataset. This supports the previous histograms being right skewed\")\n",
        "# Make a smaller DataFrame with only the relevant columns\n",
        "df_relevant = df[relevant_columns]\n",
        "\n",
        "# Create the correlation matrix\n",
        "correlation_matrix = df_relevant.corr()\n",
        "\n",
        "# Plot the heatmap\n",
        "print(\"\\nCorrelation Heatmap:\\n\")\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap=\"coolwarm\", fmt=\".2f\", linewidths=0.5)\n",
        "plt.title('Correlation Heatmap of Relevant Variables', fontsize=16)\n",
        "plt.xticks(rotation=45)\n",
        "plt.yticks(rotation=0)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"Insight: The heatmap reveals strong positive correlations between PM2.5 and PM10 (0.88) and between PM2.5 and CO (0.80), indicating that these pollutants \\n often rise and fall together. Temperature shows a strong negative correlation with pollution levels.\")\n",
        "\n",
        "# Statistical Summary\n",
        "df[relevant_columns].describe()\n",
        "\n",
        "# Scatter plots\n",
        "print(\"\\nScatter Plots:\\n\")\n",
        "\n",
        "# Set the style\n",
        "sns.set(style=\"whitegrid\", font_scale=1.2)\n",
        "\n",
        "# Create the figure and axes\n",
        "fig, axs = plt.subplots(2, 2, figsize=(16, 10))  # 2 rows x 2 cols\n",
        "\n",
        "# Plot 1: PM2.5 vs PM10\n",
        "sns.regplot(x='PM2.5', y='PM10', data=df, ax=axs[0, 0], scatter_kws={'s': 20}, line_kws={'color': 'red'})\n",
        "axs[0, 0].set_title('PM2.5 vs PM10')\n",
        "\n",
        "# Plot 2: PM2.5 vs CO\n",
        "sns.regplot(x='PM2.5', y='CO', data=df, ax=axs[0, 1], scatter_kws={'s': 20}, line_kws={'color': 'red'})\n",
        "axs[0, 1].set_title('PM2.5 vs CO')\n",
        "\n",
        "# Plot 3: CO vs NO2\n",
        "sns.regplot(x='CO', y='NO2', data=df, ax=axs[1, 0], scatter_kws={'s': 20}, line_kws={'color': 'red'})\n",
        "axs[1, 0].set_title('CO vs NO2')\n",
        "\n",
        "# Plot 4: CO vs PM10\n",
        "sns.regplot(x='CO', y='PM10', data=df, ax=axs[1, 1], scatter_kws={'s': 20}, line_kws={'color': 'red'})\n",
        "axs[1, 1].set_title('CO vs PM10')\n",
        "\n",
        "# Tidy layout\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"Insight: The scatterplots provided insight into the relationships between key pollutant variables:\"\n",
        "\"\\n \\n PM2.5 vs PM10: A strong positive linear relationship was observed, indicating that as PM10 levels increase, PM2.5 levels tend to increase similarly. This is expected, \\n as both represent particulate matter of different sizes and often originate from similar sources.\"\n",
        "\"\\n \\n PM2.5 vs CO: A moderate positive correlation was noted, suggesting that higher concentrations of particulate matter are often associated with increased carbon monoxide \\n levels, likely due to combustion-related pollution (e.g., traffic emissions).\"\n",
        "\"\\n \\n CO vs NO2: Another moderate positive relationship was observed, implying that CO and NO2 may share common emission sources such as vehicle exhaust or industrial activities.\"\n",
        "\"\\n \\n SO2 vs PM2.5: The relationship appeared weaker and more scattered, indicating that sulfur dioxide levels are less directly related to PM2.5 concentrations in this dataset,\\n possibly due to varying sources or atmospheric reactions affecting SO2 concentrations independently.\"\n",
        "\"\\n \\n Overall, the scatterplots highlight that several pollutants are interrelated, particularly particulate matter and gaseous pollutants associated with combustion activities. \\n However, not all pollutants display strong linear relationships, reflecting the complex nature of air pollution dynamics.\")"
      ],
      "metadata": {
        "id": "qwOGiDWu9QvN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testing the models and splits for highest accuracy"
      ],
      "metadata": {
        "id": "VmBVhUGB-Y4R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# 1. Load your cleaned dataset\n",
        "df = pd.read_csv('merged_cleaned_PRSA_data.csv')\n",
        "\n",
        "# 2. Select relevant features\n",
        "features = ['PM10', 'SO2', 'NO2', 'CO', 'O3', 'TEMP', 'PRES', 'DEWP', 'RAIN', 'WSPM']\n",
        "target = 'PM2.5'\n",
        "\n",
        "X = df[features]\n",
        "y = df[target]\n",
        "\n",
        "# 3. Define the models\n",
        "models = {\n",
        "    \"Linear Regression\": LinearRegression(),\n",
        "    \"Decision Tree\": DecisionTreeRegressor(random_state=42),\n",
        "    \"Random Forest\": RandomForestRegressor(random_state=42),\n",
        "    \"KNN\": KNeighborsRegressor(n_neighbors=5)\n",
        "}\n",
        "\n",
        "# 4. Define the splits\n",
        "splits = {\n",
        "    \"80/20\": 0.2,\n",
        "    \"70/30\": 0.3,\n",
        "    \"90/10\": 0.1\n",
        "}\n",
        "\n",
        "# 5. Evaluate each model on each split\n",
        "for split_name, test_size in splits.items():\n",
        "    print(f\"\\n--- Train/Test Split {split_name} ---\")\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42)\n",
        "\n",
        "    # Feature scaling (for models that benefit, like KNN)\n",
        "    scaler = StandardScaler()\n",
        "    X_train_scaled = scaler.fit_transform(X_train)\n",
        "    X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "    for name, model in models.items():\n",
        "        if name == \"KNN\":\n",
        "            model.fit(X_train_scaled, y_train)\n",
        "            preds = model.predict(X_test_scaled)\n",
        "        else:\n",
        "            model.fit(X_train, y_train)\n",
        "            preds = model.predict(X_test)\n",
        "\n",
        "        r2 = r2_score(y_test, preds)\n",
        "        mae = mean_absolute_error(y_test, preds)\n",
        "        mse = mean_squared_error(y_test, preds)\n",
        "        rmse = np.sqrt(mse)\n",
        "\n",
        "        print(f\"\\n{name}\")\n",
        "        print(\"R² Score:\", round(r2, 2))\n",
        "        print(\"Mean Absolute Error (MAE):\", round(mae, 2))\n",
        "        print(\"Mean Squared Error (MSE):\", round(mse, 2))\n",
        "        print(\"Root Mean Squared Error (RMSE):\", round(rmse, 2))\n"
      ],
      "metadata": {
        "id": "DdRs10qk-dkG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Redone modeling section to save the most accurate models"
      ],
      "metadata": {
        "id": "vKnJXIj-_Mu9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import joblib\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "\n",
        "\n",
        "# Load in the clean data\n",
        "df = pd.read_csv('merged_cleaned_PRSA_data.csv')\n",
        "# --- 1. Select Features and Target ---\n",
        "\n",
        "# X = features (input), y = target (what we're predicting)\n",
        "relevant_columns = [\n",
        "    'PM10', 'SO2', 'NO2', 'CO', 'O3',\n",
        "    'TEMP', 'PRES', 'DEWP', 'RAIN', 'WSPM'\n",
        "]\n",
        "X = df[relevant_columns]\n",
        "y = df['PM2.5']\n",
        "\n",
        "# Split into Training and Testing Sets\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Feature Scaling\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Model 1: Linear Regression\n",
        "\n",
        "linreg = LinearRegression()\n",
        "linreg.fit(X_train_scaled, y_train)\n",
        "\n",
        "y_pred_linreg = linreg.predict(X_test_scaled)\n",
        "\n",
        "# Model 2: Decision Tree Regressor\n",
        "\n",
        "dtr = DecisionTreeRegressor(random_state=42)\n",
        "dtr.fit(X_train, y_train)  # Decision Trees don't need scaling\n",
        "\n",
        "y_pred_dtr = dtr.predict(X_test)\n",
        "\n",
        "# Evaluation Function\n",
        "\n",
        "def evaluate_model(true, predicted, model_name):\n",
        "    print(f\"\\n--- {model_name} ---\")\n",
        "    print(f\"R² Score: {r2_score(true, predicted):.2f}\")\n",
        "    print(f\"Mean Absolute Error (MAE): {mean_absolute_error(true, predicted):.2f}\")\n",
        "    print(f\"Mean Squared Error (MSE): {mean_squared_error(true, predicted):.2f}\")\n",
        "    print(f\"Root Mean Squared Error (RMSE): {np.sqrt(mean_squared_error(true, predicted)):.2f}\")\n",
        "\n",
        "#  Evaluate Both Models\n",
        "\n",
        "evaluate_model(y_test, y_pred_linreg, \"Linear Regression\")\n",
        "evaluate_model(y_test, y_pred_dtr, \"Decision Tree Regressor\")\n",
        "\n",
        "# Model 3: Random Forest Regressor\n",
        "rf_model = RandomForestRegressor(random_state=42)\n",
        "rf_model.fit(X_train, y_train)\n",
        "rf_preds = rf_model.predict(X_test)\n",
        "\n",
        "print(\"\\n--- Random Forest Regressor ---\")\n",
        "print(\"R² Score:\", round(r2_score(y_test, rf_preds), 2))\n",
        "print(\"Mean Absolute Error (MAE):\", round(mean_absolute_error(y_test, rf_preds), 2))\n",
        "print(\"Mean Squared Error (MSE):\", round(mean_squared_error(y_test, rf_preds), 2))\n",
        "print(\"Root Mean Squared Error (RMSE):\", round(np.sqrt(mean_squared_error(y_test, rf_preds)), 2))\n",
        "\n",
        "# Model 4: K-Nearest Neighbors Regressor\n",
        "knn_model = KNeighborsRegressor(n_neighbors=5)\n",
        "knn_model.fit(X_train, y_train)\n",
        "knn_preds = knn_model.predict(X_test)\n",
        "\n",
        "print(\"\\n--- K-Nearest Neighbors Regressor ---\")\n",
        "print(\"R² Score:\", round(r2_score(y_test, knn_preds), 2))\n",
        "print(\"Mean Absolute Error (MAE):\", round(mean_absolute_error(y_test, knn_preds), 2))\n",
        "print(\"Mean Squared Error (MSE):\", round(mean_squared_error(y_test, knn_preds), 2))\n",
        "print(\"Root Mean Squared Error (RMSE):\", round(np.sqrt(mean_squared_error(y_test, knn_preds)), 2))\n",
        "\n",
        "# save the models and scaler for the app\n",
        "joblib.dump(scaler, 'scaler.pkl')\n",
        "joblib.dump(rf_model, 'random_forest_model.pkl')\n",
        "joblib.dump(knn_model, 'knn_model.pkl')\n"
      ],
      "metadata": {
        "id": "HgEwXJjp_UDd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}